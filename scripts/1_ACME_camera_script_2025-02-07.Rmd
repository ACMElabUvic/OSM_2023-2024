---
title: "ACME camera script 9-2-2024"
author: "Marissa A. Dyck and Aidan M. Brushett"
date: "2024-02-09"
output:
  pdf_document:
    toc: true
  html_document:
    theme: journal
    toc: true
    toc_float: true
---
 
> IMPORTANT the first two chunks of this r markdown file **after** the r setup allow for plot zooming, but it also means that the html file must be opened in a browser to view the document properly. When it knits in RStudio the preview will appear empty but the html when opened in a browser will have all the info and you can click on each plot to Zoom in on it. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

These chunks (only visible in RStudio) allow for plot zooming once knitted and opened in browser, can delete if you don't want in your R markdown doc

```{css zoom-lib-src, echo = FALSE}
script src = "https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"
```

```{js zoom-jquery, echo = FALSE}
 $(document).ready(function() {
    $('body').prepend('<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>');
    // onClick function for all plots (img's)
    $('img:not(.zoomImg)').click(function() {
      $('.zoomImg').attr('src', $(this).attr('src')).css({width: '100%'});
      $('.zoomDiv').css({opacity: '1', width: 'auto', border: '1px solid white', borderRadius: '5px', position: 'fixed', top: '50%', left: '50%', marginRight: '-50%', transform: 'translate(-50%, -50%)', boxShadow: '0px 0px 50px #888888', zIndex: '50', overflow: 'auto', maxHeight: '100%'});
    });
    // onClick function for zoomImg
    $('img.zoomImg').click(function() {
      $('.zoomDiv').css({opacity: '0', width: '0%'}); 
    });
  });
```



# Before you begin

## Notes

A few notes about this script.

If you are running this with the 2023-2024 data make sure you download the whole (OSM_2023-2024 GitHub repository)[https://github.com/ACMElabUvic/OSM_2023-2024] from the ACMElabUvic GitHub. This will ensure you have all the files, data, and proper folder structure you will need to run this code and associated analyses.

Also make sure you open RStudio through the R project (OSM_2023-2024.Rproj) this will automatically set your working directory to the correct place (wherever you saved the repository) and ensure you don't have to change the file paths for some of the data. 

If you have question please email the most recent author, currently   

Marissa A. Dyck   
Postdoctoral research fellow    
University of Victoria    
School of Environmental Studies     
Email: [marissadyck17@gmail.com](marissadyck17@gmail.com)     

**and**

Aidan M. Brushett  
M.Sc. student
University of Victoria    
School of Environmental Studies     
Email: [aidanbrushett@uvic.ca](aidanbrushett@uvic.ca)     

## Netdrive access

This script relies on the user having access to the ACME lab Netdrive (you can view the .html output of this file if you don't have access and just want to see what the script did).

Helpful instructions for connecting to and navigating the Netdrive can also be found here: [https://docs.google.com/document/d/1Z72IrlIXO8MUHCoVztcMrMdL10R2tHrHThEgfow1Cu0/edit ](https://docs.google.com/document/d/1Z72IrlIXO8MUHCoVztcMrMdL10R2tHrHThEgfow1Cu0/edit).


## R and RStudio

Before starting you should ensure you have the latest version of R and RStudio downloaded. This code was generated under R version 4.2.3 and with RStudio version 2024.04.2+764.    

You can download R and RStudio [HERE](https://posit.co/download/rstudio-desktop/)   


## R markdown

This script is written in R markdown and thus uses a mix of coding markup languages and R. If you are planning to run this script with new data or make any modifications you will want to be familiar with some basics of R markdown.

Below is an R markdown cheatsheet to help you get started,    
[R markdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)    


## Install packages

If you don't already have the following packages installed, use the code below to install them. *NOTE this will not run automatically as eval=FALSE is included in the chunk setup (i.e. I don't want it to run every time I run this code since I have the packages installed)

```{r install packages, eval=FALSE}

install.packages('tidyverse') 
install.packages('withr') 
```


## Load libraries

Then load the packages to your library.

```{r libraries}
rm(list=ls()) # start with a fresh global environment
library('tidyverse') # data tidying, visualization, and much more; this will load all tidyverse packages, can see complete list using tidyverse_packages()
library('withr') # used to temporarily set wd
```


## Camera operability 

We can check the length each camera was operating using the cleaned deployment data, this is important for calculating the proportional presence/absences for analysis later on so we need to make sure nothing looks inaccurate here.

Let's plot the camera operability with `ggplot()` to look at this

```{r camera operability}

# if starting from this point read in data
deploy <- read_csv('data/processed/OSM_deployment_2023.csv') %>% 
  
  # make sure site re-reads in as a factor to compare with other data sets
  mutate(site = as.factor(site))

# create graph of camera operability

ggplot(deploy, aes(color = array))+
  
  geom_segment(aes(x = start_date, 
                   xend = end_date,
                   y = site, 
                   yend = site)) +
  
  theme(axis.text = element_text(size = 6))

```


## Load in the cleaned timelapse data for this year. 

Error-checking is done in script 0 now, so code from the 2022-2023 version of this script related to data inspection and manipulation has been removed. 

``` {r}
OSM_2023_data <- read_csv("./data/processed/OSM_timelapse_2023.csv",
                          col_types = cols(datetime = col_datetime(),
                                           site = col_character(),
                                           total = col_integer(),
                                           group_count = col_integer(), 
                                           comments = col_character(),
                                           species = col_factor(),
                                           otherspecify = col_character(),
                                           snow = col_factor(),
                                           empty = col_logical()
                                           ))

str(OSM_2023_data)
```

## Summaries

Some summaries we may need for reports etc. 

```{r summaries}

# mammal specific 
levels(OSM_2023_data$species)

# make vector of mammals
mammal_species <- c('Black bear',
                    'Caribou',
                    'Coyote',
                    'Fisher',
                    'Grey wolf',
                    'Lynx',
                    'Moose',
                    'Red fox',
                    'White-tailed deer',
                    'Snowshoe hare',
                    'Red squirrel',
                    'Marten',
                    'Striped skunk',
                    'Cougar',
                    'Porcupine',
                    'Short-tailed weasel',
                    'Otter',
                    'Beaver',
                    'Wolverine',
                    'Long-tailed weasel')

# how many photos of just mammals
all_mammals <- OSM_2023_data %>% 
  
  filter(species %in% mammal_species)



# focal species 
focal_species <- c('Black bear',
                   'Caribou',
                   'Coyote',
                   'Fisher',
                   'Grey wolf',
                   'Lynx',
                   'Moose',
                   'Red fox',
                   'White-tailed deer')

# detections of focal species
focal_mammals <- OSM_2023_data %>% 
  
  filter(species %in% focal_species)
```


## Independent detections

For the models we want to run, we need data for the independent detections of each species at each site. We've defined independent detections as those at least 30 minutes apart. 

The data manipulation and loop below will do this for us.

```{r independent detections}

# we will continue working with the OSM_2023_data but you could start here and import the data again if needed but you'd have to change the structure of a couple variables that get re-read in wrong after the export and import process. Can uncomment and use the code below to do that.

# can select code chunk and use command + shift + c to uncomment or comment a large portion of code

# OSM_2022_data_fixed <- read_csv('data/processed/OSM_2022_timelapse.csv')
# 
# # ignoring parsing issues warning, this is just referring to some columns it's expecting logical data for and they contain characters.
# 
# # check internal structure, even though we specified everything above with the fixed data when we export and import R often reads the variables in wrong again
# str(OSM_2022_data_fixed) 
# 
# # datatime read correctly but we will need to change site and species to factors again
# 
# OSM_2022_data_fixed <- OSM_2022_data_fixed %>% 
#   
#   mutate(species = as.factor(species),
#          site = as.factor(site))
  

# set the independent detection threshold to 30 minutes
mins <- 30 

# prep the data for calculating independent detections
OSM_2023_det <- OSM_2023_data %>% 
  
  # remove rows with no species info
  drop_na(species) %>% 
  
  # We want to avoid using empty group images where no animal is actually present. This could falsely alter independent detections. 
  filter(empty == FALSE) %>% 
  
  # select only variables of interest
  select(array,
         site,
         species,
         datetime,
         month,
         year) %>% 
  
  # now we need to create a new variable called timediff
  # first make sure data are arrange in proper order
  arrange(site, species, datetime) %>% # this will NOT work if not in correct order (early-late date)
  
  # create groups for each species at each site
  group_by(species, site) %>%
  
  # create new variable timediff that will calculate the difference 
  mutate(timediff = as.numeric(difftime(datetime,lag(datetime),
                                        units = "mins")),
        # Then, create an event_ID that groups detections and identifies when photos are >30min apart. 
        event_id = cumsum(if_else(is.na(timediff) | timediff > mins,  # Create episode IDs to distinguish detections 
                                  1, 
                                  0))
        )

# now create a new data frame with a single row for each event. 
# The timestamp corresponds to the EARLIEST image in the event. Other code could be added 
OSM_2023_det_ind <- OSM_2023_det %>% 
  
  group_by(array, species, site, event_id, year, month) %>%
  
  summarize(datetime = min(datetime))

## This code can be subbed in to pull the first and last timestamps. Useful for calculating COARSE event duration. 
#  summarize(event_start = min(datetime), event_end = max(datetime))
```

### Save independent detections

Let's also save this data file for later

```{r save detection data}

write_csv(OSM_2023_det_ind,
          'data/processed/OSM_ind_det_2023.csv')

```


## Graphs

### Indpendent detections for mammals

Now lets create a few quick figures to look at the detection data

```{r graph mammal detections}

# Data visualization independent detections---------------------------------------------

# read in saved detection data if starting here
detections <- read_csv('data/processed/OSM_ind_det_2023.csv') %>% 
  
  # change site, species and event_id to factor
  mutate_if(is.character,
            as.factor)

# check number of different species
levels(OSM_2023_det_ind$species)

# create a vector of the list of mammals to use for quick data visualization/exploration. Could also create a list of species that aren't useful if that is shorter and use filter(!species %in% OBJECTNAME) but for this example the vectors were about the same length
mammals <- c('White-tailed deer',
             'Black bear',
             'Snowshoe hare',
             'Moose',
             'Coyote',
             'Fisher',
             'Red squirrel',
             'Striped skunk',
             'Grey wolf',
             'Red fox',
             'Cougar',
             'Lynx',
             'Short-tailed weasel',
             'Porcupine',
             'Beaver',
             'Martin',
             'Wolverine',
             'Caribou',
             'Long-tailed weasel')

# remove NAs and select just images with mammals first then pipe new data into ggplot
det_graph <- detections %>% 
  
  # remove less useful species
  filter(species %in% mammals) %>% 
  
  # get the number of individual detections per species to add to graph
  group_by(species) %>% 
  
  mutate(n = n()) %>% 
  
  ungroup() %>% 
  
  ggplot(.,
         aes(x = species)) +
  
  # create bar graph of the counts of each spp in the data
  geom_bar(aes(fill = species)) +
  
  # add the number of detections above each bar using the variable n we calculated earlier
  geom_text(aes(label = n,
                y = n + 50),
            size = 4) +
  
  # change y axis label
  labs(y = 'Number of independent detections') +
  
  # change breaks for y axis
  scale_y_continuous(breaks = seq(0,3500, by = 250)) +
  
  # change theme elements
  theme(axis.text.x = element_text(angle = 90,
                                   vjust = 0.5,
                                   hjust = 1,
                                   size = 14),
        axis.title = element_text(size = 16),
        axis.ticks.x = element_blank(),
        panel.grid = element_blank()) 

# print graph
det_graph

```

If we want to save the plot for easier viewing later we can use the code below

```{r save det_graph, eval=TRUE}

# save graph as jpeg (can also save as tiff, png, pdf by changing the file extension) but don't use .tiff in the github repo it takes up too much space and causes issues
ggsave('2023_indv_det_graph.jpeg',
       det_graph,
       path = 'figures',
       width = 12,
       height = 10,
       units = 'in',
       dpi = 600)

```

### Independent detections per LU

let's also create one that graphs each LU in it's own panel using facet_wrap
```{r det graph LUs}
# let's also create one that graphs each LU in it's own panel using facet_wrap
det_plot_LUs <- detections %>% 
  
  # remove less useful species
  filter(species %in% mammals) %>% 
  
 # group by array and species to calculate dets per spp per LU
 group_by(array, species) %>% 
  
  # calculate a column with unique accounts of each species
  reframe(count = n_distinct(event_id)) %>% 
  
  # pipe to ggplot and set aesthetics mapping
  ggplot(aes(x = reorder(species, count), y = count)) +
  
  # plot as bar graph
  geom_col() +
  
  # plot each LU in own panel
  facet_wrap(vars(array)) +
  
  # add the number of detections at the end of each bar
  geom_text(aes(label = count),
            color = "black",
            size = 3,
            hjust = 0.2,
            vjust = -0.3) +
  
  # label x and y axis with informative titles
  labs(x = 'Species',
       y = 'Number of Independent (30 min) Detections') +
  
  # add title to plot with LU name
  
  ggtitle("LU21 Detections")+
  
  # set the theme
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 90,
                                   vjust = 0.5,
                                   hjust = 1,
                                   size = 12))

# view plot
det_plot_LUs
```

We can also save this plot if we want it for reports etc. 

```{r}
# save this plot

ggsave('figures/OSM_ind_det_per_LU_2023.jpg',
       det_plot_LUs,
       dpi = 600,
       width = 10,
       height = 12,
       units = 'in')
```










------- END AIDAN CODE SO FAR. 











# Covariate data

## Import covariate data

These data files have a similar format so we will read them in together using the `map()` function in the *purrr* package. Reminder, the `map()` function let's us perform iterations. The `~.x` after the function is a placeholder that refers to the data before the last pipe (e.g. the .csv files we supplied) and all operations within the `map()` will be performed on all of these objects. 

We are doing a few data manipulation steps here to make the data checks easier since I know how I want some of the columns/entries formatted.

The code below will 

1. Provide the path and filenames of the two csv files we need    
2. Read them in and specify the column types    
3. Set the column names to lowercase, remove the feature_ty prefix in each column, and replace dashes with underscores    
4. then create two additional columns, array and camera from the site column information    
5. finally set the names of each item in the list as HFI for human footprint inventory and VEG for and landcover data   

```{r import covariate data}

# these data files have a similar format so we will read them in together using the map() function in the purrr package

covariate_data <-    
  # provide file path (e.g. folders to find the data)
  file.path('data/raw',
            
            # provide the file names
            c('OSM_LU01_LU13_LU15_LU21_HFI_2022_2024-04-19.csv',
              'OSM_LU01_LU13_LU15_LU21_VEG_2022_2024-04-19.csv')) %>%
  
  # use purrr map to read in files, the ~.x is a placeholder that refers to the object before the last pipe (aka the list of data we are reading in) so all functions inside the map() after ~.x will be performed on all the objects in the list we provided
  map(~.x %>%
        read_csv(.,
                 
                 # specify how to read in the various columns
                 col_types = cols(Site = col_factor(),
                                  BUFF_DIST = col_integer(),
                                  .default = col_number())) %>%
        
        
        
        # set the column names to lower case
        set_names(
          names(.) %>% 
            tolower())) %>% 
            
           
  # set the names of the two files in the list, if you don't run this they will be named numerically (e.g. [1], [2]) which can get confusing
  purrr::set_names('HFI',
                   'VEG')

# will get a warning about parsing issues, don't panic it is fine

```

## Data checks 

### Structure

Even though we set some of the columns to read in as a specific type in the data import step it's always a good idea to check internal structure. 

```{r}
str(covariate_data)
```
From a quick glance everything looks good. 

### Sites

Now let's check that all the sites are accounted for, there should be 155 just like with the timelapse data

```{r covariates site names}

# check that the sites are all there and entered correctly, there should be 155

# since the data sets are in a list we need to call the list first, then the data name in the list, then the column name
levels(covariate_data$HFI$site)
levels(covariate_data$VEG$site)
```

There are 155 for both data sets and I don't see any glaring issues but we can use a function in R to check that these match perfectly.

*If you are getting a NULL output for any of the `setdiff()` lines of code, then you need to make sure that site is actually a factor in both the data sets.* Sometimes data manipulation steps change how the variable is reading in R so you made need to fix this. 

```{r covariates setdiff}
# there are 155 for both and don't see any glaring issues but let's check that all these site names match each other using the setdiff function
setdiff(levels(covariate_data$VEG$site),
        levels(covariate_data$HFI$site))

# no mismatches

# we need to check that they also match the osm_2022_Det
setdiff(levels(detections$site),
        levels(covariate_data$HFI$site))

# [1] "LU12_51" "LU15_35"
# there seems to be two site names that are different between the covariate data sets and the detection data

# reverse the order to see which two are extras in the covariate data
setdiff(levels(covariate_data$HFI$site),
        levels(detections$site))

# [1] "LU13_51" "LU13_35" it looks like the landscape units might have gotten typed in wrong. # checked with original data and these are the correct ones 

```

> We fixed the site issue in the raw data since it's important that the site names are correct so you won't see a difference between the data sets now but I've left the code as an example.

### Column names

We should check that the column names all look good, there are a ton for the HFI data frame so we won't look at each of the features individually but check that the general formatting/naming is okay

```{r HFI names}

names(covariate_data$HFI)
```
These look okay but we should replace the dash '-' with and underscore '_' to match formatting of other files and because it's easier for R to work with.

We also want to add array and camera columns which we can do using the site data.

Let's check the VEG data too

```{r VEG names}

names(covariate_data$VEG)
```
We also need to add array and camera columns which we can do using the site data.


### NAs

Let's check the summary for any NAs that shouldn't be in the data, mostly we are looking for NAs in the site or buff_dist columns

```{r}

summary(covariate_data$HFI)
summary(covariate_data$VEG)
```
Everything looks good!
 

## Data formatting
 
As with the previouos sections this section will likely change each year but offers a good starting point, and I do all the data manipualtion in one code chunk but run each portion individually as I build the chunk to make sure it's working.

This code will do the following data formatting on both files simultaneously using purrr::map

1. Change the column names - remove the feature_ty prefix in each column and replace dashes with underscores    
2. then create two additional columns, array and camera from the site column information    
5. finally set the new variables as factors

```{r format covariate data}

 covariate_data_fixed <- covariate_data %>% 
  
  map(
    ~.x %>% 
      
      set_names(
        names(.) %>% 
          # remove the FEATURE_TY in front of all the column names because it's not helpful
          str_remove(pattern = "feature_ty") %>% 
          
          # replace the '.' with '_' in the feature column names
          str_replace_all(pattern = '-', # provide the character pattern to look for (if you don't keep the \\ it won't work)
                          replacement = '_')) %>%  # what you want the pattern to be replaced with
      
      separate_wider_delim(site,
                           delim = '_',
                           names = c('array',
                                     'camera'),
                           cols_remove = FALSE) %>% 
      
      # specify format of new columns
      mutate(
        array = as.factor(array),
        camera = as.factor(camera)
      ))
```

Now let's recheck the data, data structure, and the sites with the deployment data, you can run each of these individually or all at once and review each one

```{r covs data fixed site check}

# check structure of variables
str(covariate_data_fixed)

# take a look at the column names
names(covariate_data_fixed$HFI)
names(covariate_data_fixed$VEG)

```


## Join covariate data

Now we need to join the HFI and VEG files together

```{r join covariate data}
covariates_all <- covariate_data_fixed$HFI %>% 
  
  #use full join in case any issues with missing observations but we should be good since we checked the site names
  full_join(covariate_data_fixed$VEG,
            by = c('array', 'camera', 'site', 'buff_dist'))


head(covariates_all)

```


## Finish covariates data

### Save data

Let's also save this for future use
 
```{r save joined covariate data}

# save joined data 
write_csv(covariates_all,
          'data/processed/OSM_covariates_2022.csv')

```

We may want each buffer to have it's own column for each variable (e.g. create a wide format of this data) for modeling purposes, we can do that with the `pivot_wider()` function. 

```{r wide format covariates}
# we also may want to pivot wider so that each column is for a different buffer for modeling purposes, we can use pivot wider to do this

covariates_all_wide <- covariates_all %>% 
  
  pivot_wider(.,
              names_from = buff_dist,
              values_from = c(vegetated_edge_roads:lc_class230))

head(covariates_all_wide)
```

Let's also save this data

```{r save wide format covariates}

# save wide format data 
write_csv(covariates_all_wide,
          'data/processed/OSM_covariates_wide_2022.csv')
```








----------- AIDAN HAS NOT TOUCHED COVARIATE CODE YET










# Response metrics

there are several response metrics we can calculate, the ones we will cover here are. 

 1. Total independent detections per species/site
 2. Presence/absence per species/site
 3. Proportion of monthly detections

> Generally we only need #3 (proportional monthly detections) but we provided data for ES 482/582 class and wanted them to have multiple responses metrics to choose from for modeling purposes. 

## Data

For this we need the deployment and independent detection data we created earlier, if you are still working through this script its the 'deploy_fixed' & 'detections' objects

```{r response metric data}
# deploy
deploy <- read_csv('data/processed/OSM_deployment_2023.csv')


# detections
detections <- read_csv('data/processed/OSM_ind_det_2023.csv') %>% 
  
  # change site, species and event_id to factor
  mutate_if(is.character,
            as.factor)
```

For plotting and formatting proportional monthly detections we need to create a subset of the species in the detections data to just include several focal species we are interested in
```{r focal species}

# create a list of focal species for filtering the data/plots
 focal_species <- c('Black bear',
                    'Caribou',
                    'Cougar',
                    'Coyote',
                    'Fisher',
                    'Grey wolf',
                    'Lynx',
                    'Moose',
                    'Red fox',
                    'White-tailed deer',
                    'Wolverine',
                    'Snowshoe hare', # this one was added for ES 482/582 class data, we don't normally use for OSM reports
                    'Red squirrel' # this one was added for ES 482/582 class data, we don't normally use for OSM reports
                    )
```


## 1. Total independent detections 

The first response metric we will calculate is the total number of independent detections per species per site. 

To do this we use the detections data we created earlier from the raw Timelapse data. We need to group by site and species and then we can use the `summarise()` function with the `n()` function to count the total detections.

After that we ungroup the data so if we run an analysis or make a plot it doesn't stay grouped and then we use the `pivot_wider()` function to make a column for each species and a row for each site.

Finally we need to replace any NAs with zeros (the `n()` function function won't insert a zero if there aren't any observations to count so these NAs are indeed zeros)
```{r total ind det}

total_detections <- detections %>% 
  
  # group by site and species to count detections
  group_by(site, species) %>% 
  
  # use summarise to count detections per species per site
  summarise(detections = n()) %>% 
  
  ungroup() %>% 
  
  pivot_wider(names_from = species,
              values_from = detections) %>% 
  
  # replace NAs with 0 in all species columns
  mutate(across(
    where(is.numeric),
    ~ replace_na(., 0)))
  
```

Now that this data is formatted we should save it to the data/processed folder for use later

```{r save total ind det}

# save csv file to processed data folder 
write_csv(total_detections,
          'data/processed/OSM_total_detections_2023.csv')
```

We can also plot this data to see what it looks like

In the code below we create an object called site_detections_plot where we pipe the total detection data into the `ggplot()` function after doing some formatting to make it plot

```{r plot total ind det}

site_detections_plot <-  total_detections %>% 
  
  # we need to pivot longer to create species column again for plotting
  pivot_longer(cols = 2:37,
               names_to = 'species',
               values_to = 'detections') %>% 
  
  # remove less useful species using a list created in the 'Graph independent detections' section
  filter(species %in% mammals) %>% 
  
  # pipe into ggplot function
ggplot(.,
       aes(x = site,
           y = detections)) +
  
  geom_col() +
  
  # use facet wrap to make separate plots for each species 
  facet_wrap(vars(species)) +
  
  # shift axis text to 90 degrees so site name are readable
  theme(axis.text.x = element_text(angle = 90,
                                   size = 3),
        axis.ticks.x = element_blank())


# view plot
site_detections_plot

```

This is not the most readable plot because some species are skewing the x axis really high but it works for exploratory purposes.

If we want to save the plot for easier viewing later we can use the code below

```{r save plot total ind det, eval=TRUE}

# save graph as jpeg (can also save as tiff, png, pdf by changing the file extension)
ggsave('OSM_total_detections_site_2023.jpeg',
       site_detections_plot,
       path = 'figures',
       width = 12,
       height = 10,
       units = 'in',
       dpi = 600)

```


## 2. Presence/absences

A second response metric we may want to use is simply presence/absence data. Here we can use the total_detections data and replace any values greater than 0 with 1s to create a binary variable. 

```{r presence/absence}
# we can use the data from above to create a similar response metric of simply presences and absences per species per site

species_presence <- total_detections %>% 
  
  # replace all values above 0 with 1s
  mutate_if(is.numeric, 
            ~1 * (. > 0))

# now we have presence absence data for all species
```

We can save this to data/processed

```{r save presence/absences}
# save csv file to processed data folder 
write_csv(species_presence,
          'data/processed/OSM_total_presence_absence_2023.csv')
```

We may also want to plot this similarly to the total detections per site to look at the data easier

As before we create a new object with our plot title name and then pipe the data after some formatting into the `ggplot()` function.
```{r plot presence/absence}
# plot this data in ggplot
species_presence_plot <- species_presence %>% 
  
  # first we need to pivot the data longer again so we have a species column for plotting
  pivot_longer(cols = 2:37,
               names_to = 'species',
               values_to = 'presence') %>% 
  
  # remove less useful species using a list created in the 'Graph independent detections' section
  filter(species %in% mammals) %>% 
  
  # pipe into ggplot function
ggplot(., 
       aes(x = site, y = presence)) +
  
  # use geom_jitter instead of geom_point so we can shift points on y-axis to make them easier to view
  geom_jitter(shape = 16,
              size = 1.5,
              width = 0,
              height = 0.05,
              alpha = 0.5) +
  
  # use facet_wrap to make separate plots for each species so data is easier to look at
  facet_wrap(vars(species)) +
  
  # shift axis text to 90 degrees so site name are readable
  theme(axis.text.x = element_text(angle = 90,
                                   size = 3))



# view plot
species_presence_plot
```

If we want to save the plot for easier viewing later we can use the code below

```{r save plot presence/absence, eval=TRUE}

# save graph as jpeg (can also save as tiff, png, pdf by changing the file extension)
ggsave('OSM_total_presence_absence_2023.jpeg',
       species_presence_plot,
       path = 'figures',
       width = 12,
       height = 10,
       units = 'in',
       dpi = 600)
```

## 3. Proportion monthly detections

We need to use the deployment data to determine how many days each camera was active for

First we create a new data frame from the deploy_fixed (deployment data frame that's been cleaned) with some of the same columns and a new column 'day' that goes from the start date to the end date of each camera deployment and increases by intervals of 1 (for each day the camera was active). We use this to calculate the number of days/month each camera was active for and create a new variable for this called days_month. and then we create another column based on the days/month the camera was active that classifies each month of data for a camera as keep or remove based on whether there were at least 15 active camera days in that month. This is because we don't want to estimate monthly data for a camera that was working less than half the time. 

```{r prop month detections step 1: days active}
deploy_active <- deploy %>%
  
  drop_na(start_date, end_date) %>% # remove NA rows. There is one, from a camera that was not retrieved :(
  
  rowwise() %>%
  
  # create a list sequence of dates for each site from the first to the last day of operability
  mutate(date = list(seq(from = date(start_date), to = date(end_date), by="day"))) %>% 
    
  unnest(date) %>%
  
  mutate(year = year(date),
         month = month(date))
```

We also need to modify the OSM operating dataframe to account for days where the camera was fully covered by snow or otherwise obstructed (cases where `cameramalfunction == *'Fully obscured'*`). Let's identify those from the raw timelapse data and remove them from `osm_operating`.

```{r}
deploy_obscured <- OSM_2023_data %>%
  
  filter(cameramalfunction == 'Fully obscured') %>%
  
  mutate(date = as.Date(datetime)) %>%
  
  select(array, site, date) %>% 
  
  distinct()
```

Then, remove those dates from `deploy_active`. 

```{r}
deploy_active <- deploy_active %>%
  
  anti_join(deploy_obscured)
```

Now we calculate the total number of months each camera was active for. We will impose an additional restraint that 'active' months must have been operating for at least 15 days/month. We will use this data again later.

```{r prop month detections step 2: months active}
# calculate the total number of months each camera was active for including only those active for >15 days/month or the 'keep' values
deploy_active_months <- deploy_active %>%

  group_by(array, site, year, month) %>%
  
  summarize(days_active = n()) %>%
  
  filter(days_active >= 15) %>% # retain months where there are >15 days of data only.

  select(-days_active)
# we will use this data later

```

Now that we have identified cameras that were not active long enough each month to reliably extract data from. Let's merge our detection data into this dataframe of camera operability.

First, organize our independent detections into monthly bins. These will be the months of true presence (presence==1) in our final dataset. 

```{r}
monthly_presence <- detections %>%
  
  distinct(array, site, species, month, year) %>% # keep one copy of detections for each month/year
  
#  filter(month %in% all_operating_months$month) %>% # retain only the data for weeks with sufficient number of days >4. The rest we don't want. 
  mutate(presence = 1)
```

Second, let's merge our monthly presence into the list of *all* operating months with >15 days. Where NAs are introduced into the *presence* column, there was no detection of the given species in that month. We will replace these NAs with "0" to denote 'true' absence. 

```{r}
monthly_presenceabsence <- deploy_active_months %>%
  
  crossing(species = unique(as.character(monthly_presence$species))) %>%
  
  left_join(monthly_presence, by = c("array", "site", "month", "species", "year")) %>%
  
  mutate(presence = replace_na(presence, 0), # Fill missing detections with 0
         species = as.factor(species))
```

Save the presence-absence dataset:

```{r}
write_csv(monthly_presenceabsence,
          './data/processed/OSM_monthly_presence_absence_2023.csv')
```

Then from the data we keep we can create a new data frame that has 1 row per camera and a column for each species indicating how many of the active months each species was detected at that camera.

```{r prop month detections step 3: presences}

# now that we have  identified cameras that were not active long enough each month to reliably extract data from we can use that column to remove this data from the detections data frame

proportional_detections <- monthly_presenceabsence %>% 
  
  # group by site and species to create data frame with one row per site x species combo
  group_by(site, array, species) %>% 
  
  summarise(
    months_present = sum(presence > 0),
    months_active = n(),
    .groups = "drop"
  ) %>%

  # filter to only species in the list of focal species we created earlier
  # NOTE when we do this we lose 3 sites because there weren't any of these species detected at those sites during months where the camera was active >= 15 days/month
  filter(species %in% focal_species) %>% 
  
  # pivot the data wider so there is a column for each species and 1 row per site
  pivot_wider(names_from = species,
              values_from = months_present) %>%

  # replace NAs with zeros in all species columns
  mutate(across(
    where(is.numeric),
    ~ replace_na(., 0))) %>% 

  # ensure all species columns are numeric not integer
  mutate_if(is.integer,
            as.numeric)
```

Now we can run the function below to create a second column for each species that will represent the number of absences (months the species was not detected) at each camera from the active months.

```{r prop month detections step 4: absences}

# run a function to create columns for absences based on presence data and how many months the camera was functioning

# first convert data to data frame not a tibble for function to work
proportional_detections <- as.data.frame(proportional_detections)

# create a vector of the species columns for the loop
# use all species columns (this value may change year to year)
cols <- 4:16

for (col in cols) {
  if (is.numeric(proportional_detections[,col]) & is.numeric(proportional_detections$months_active)) 
    {new_col_name <- paste0("absent_", colnames(proportional_detections)[col])
    proportional_detections[new_col_name] <- proportional_detections$months_active - proportional_detections[,col]
  }
}

```

We want to rename the species columns so they don't have spaces (r doesn't like spaces for column names but it was fine when they were entries in the data)

```{r prop month detections step 4: rename species columns}

# rename columns for species because R doesn't like spaces
proportional_detections <- proportional_detections %>% 
  
  # set the column names to lower case and replace the spaces with '_' (these are both personal preferences of mine)
  set_names(
    names(.) %>% 
      tolower()%>% 
      str_replace_all(pattern = ' ',
                      replacement = '_'))

```

Now we have to do a bit of final wrangling of the data to fix the bear columns because we don't want to consider the months that bears are not active in the months active columns. 

```{r prop month detections step 5: bear fix}

# fix bear data

# before we can use this data we need to adjust the columns for bears since they are hibernating we don't want to calculate their presence/absence for those inactive months

# now let's recalculate the number of active months
months_active_bears <- deploy_active_months %>% 
  
  # filter to months bears are active (April - November)
  dplyr::filter(month %in% c("4", "5", "6", "7", "8", "9", "10", "11")) %>% 
  
  # get distinct rows for each 
  distinct(site, month, year, 
           .keep_all = TRUE) %>% 
  
  # group by site
  group_by(site) %>% 
  
  # count the number of months active during bear active season and save as new column
  summarise(months_active_bears = n())
 

# now we overwrite the absent column for black bears using new info

proportional_detections_bears <- proportional_detections %>% 
  
  # join the bear active data
  left_join(months_active_bears, 
            by = 'site') %>% 
  
  # overwrite absent black bear column
  mutate(absent_black_bear = months_active_bears - black_bear) %>%
  
  # get rid of unnecessary columns for active months
  select(-c(months_active, 
            months_active_bears))

```

Finally we can save this data
```{r save prop month detections}
# save data
write_csv(proportional_detections_bears,
          'data/processed/OSM_proportional_detections_2023.csv')
```

Let's also try to plot the presence data at least for each species so we can see which species we likely have enough data for to model

```{r plot prop month dets}

# first reformat the data so species presence is a column
proportional_detection_plot <- proportional_detections_bears %>% 
  
  pivot_longer(cols = c('black_bear':'wolverine'),
               names_to = 'species_presence',
               values_to = 'months_present') %>% 
  
  select(site, array, species_presence, months_present) %>% 
  
  ggplot(aes(x = site, y = months_present, fill = array)) +
  
  # plot as bars
  geom_col() +
  
  # viridis for colorblind :)
  scale_fill_viridis_d() +
  
  # use facet wrap to generate separate plots for each species
  facet_wrap(vars(species_presence))

# view plot
proportional_detection_plot
```

If we want to save the plot for easier viewing later we can use the code below

```{r save plot prop month dets}

# save graph as jpeg (can also save as tiff, png, pdf by changing the file extension)
ggsave('OSM_proportional_detections_2023.jpeg',
       proportional_detection_plot,
       path = 'figures',
       width = 12,
       height = 10,
       units = 'in',
       dpi = 600)
```
